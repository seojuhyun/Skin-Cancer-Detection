{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIT(vision transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://haystar.tistory.com/95\n",
    "\n",
    "https://ivelopalways.tistory.com/entry/Deep-Learning-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-Vision-TransformerViT-An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale#google_vignette\n",
    "\n",
    "DATASET\n",
    "https://www.kaggle.com/competitions/isic-2024-challenge/data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# Í≤ΩÎ°ú ÏÑ§Ï†ï\n",
    "image_dir = '/home/juhyun/Downloads/isic-2024-challenge/train-image/image'\n",
    "meta_csv = '/home/juhyun/Downloads/isic-2024-challenge/train-metadata.csv'\n",
    "output_root = 'converted-dataset/train'\n",
    "\n",
    "# Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "df = pd.read_csv(meta_csv)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    isic_id = row['isic_id']\n",
    "    label = str(row['target'])\n",
    "\n",
    "    # ÏûÖÎ†• Ïù¥ÎØ∏ÏßÄ Í≤ΩÎ°ú\n",
    "    src_path = os.path.join(image_dir, f\"{isic_id}.jpg\")\n",
    "    # Ï†ÄÏû•Ìï† ÌÅ¥ÎûòÏä§ Ìè¥Îçî\n",
    "    save_dir = os.path.join(output_root, f\"class{label}\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    # Ï∂úÎ†• Ïù¥ÎØ∏ÏßÄ Í≤ΩÎ°ú\n",
    "    dst_path = os.path.join(save_dir, f\"{isic_id}.jpg\")\n",
    "\n",
    "    try:\n",
    "        shutil.copyfile(src_path, dst_path)\n",
    "    except Exception as e:\n",
    "        print(f\" Î≥µÏÇ¨ Ïã§Ìå®: {src_path} ‚Üí {dst_path} : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# Í≤ΩÎ°ú\n",
    "hdf5_path = '/home/juhyun/Downloads/isic-2024-challenge/test-image.hdf5'\n",
    "meta_csv = '/home/juhyun/Downloads/isic-2024-challenge/test-metadata.csv'\n",
    "output_dir = 'converted-dataset/test'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "df = pd.read_csv(meta_csv)\n",
    "\n",
    "# HDF5ÏóêÏÑú Ïù¥ÎØ∏ÏßÄ Ï∂îÏ∂ú\n",
    "with h5py.File(hdf5_path, 'r') as hdf:\n",
    "    for idx, row in df.iterrows():\n",
    "        isic_id = row['isic_id']\n",
    "\n",
    "        try:\n",
    "            raw = hdf[isic_id][()]  # shape: [[b'\\xff\\xd8...']]\n",
    "            if isinstance(raw, (np.ndarray, list)):\n",
    "                raw = raw[0]\n",
    "            img = Image.open(BytesIO(raw))\n",
    "            img.save(os.path.join(output_dir, f\"{isic_id}.jpg\"))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå {isic_id} Î≥ÄÌôò Ïã§Ìå®: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 288, 288])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Í∞íÎßå ÌôïÏù∏\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "x = Image.open('/home/juhyun/Desktop/digitfinal/converted-dataset/train/class0/ISIC_0015670.jpg')\n",
    "x = x.resize((288,288))\n",
    "tf_toTensor = ToTensor() \n",
    "x = tf_toTensor(x)\n",
    "x = torch.unsqueeze(x,0) #Î∞∞Ïπò ÌÅ¨Í∏∞ ÎßûÏ∂∞Ï§å\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(8,3,288,288)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 24\n",
    "N = int(288*288/(24*24)) #256\n",
    "\n",
    "\"\"\"\n",
    "Í∏∞Ï°¥Ïùò B*C*H*WÏùò Ï∞®ÏõêÏùÑ B*N*(P*P*C)Î°ú Î∞îÍøîÏ§òÏïºÌï®\n",
    "\n",
    "einopsÏùò rearrange Ìï®ÏàòÎ•º Ïù¥Ïö©ÌïòÏó¨ Ïù¥ÎØ∏ÏßÄÎ•º Ìå®ÏπòÎ°ú ÎÇòÎàÑÍ≥† flattenÏùÑ ÌïúÎ≤àÏóê ÏàòÌñâÌï† Ïàò ÏûàÎã§.\n",
    "\"\"\"\n",
    "\n",
    "patches = rearrange(x, 'b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=P, s2=P) #[1,N,P*P*c\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 144, 1728])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >> PatchEmbedding, ÌååÎùºÎØ∏ÌÑ∞Îäî ÏßÅÏ†ë Í≥ÑÏÇ∞Ìï¥ÏÑú ÏßÄÏ†ïÌï®\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels: int = 3, patch_size: int = 18, emb_size: int = 972, img_size: int = 288):\n",
    "        self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        # patch embedding\n",
    "        self.projection = nn.Sequential(\n",
    "            # using a conv layer instead of a linear one -> performance gains\n",
    "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "        \n",
    "        # nn.Parameter = ÌïôÏäµ Í∞ÄÎä•Ìïú ÌååÎùºÎØ∏ÌÑ∞Î°ú ÏÑ§Ï†ïÌïòÎäî Í≤ÉÏûÑ.\n",
    "        # Add CLS Token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1, emb_size))\n",
    "        \n",
    "        # position embedding\n",
    "        self.positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, emb_size))\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.projection(x)\n",
    "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)  # cls tokenÏùÑ xÏùò Ï≤´Î≤àÏß∏ Ï∞®ÏõêÏúºÎ°ú Î∞òÎ≥µÌï®.\n",
    "\n",
    "        # prepend the cls token to the input\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # torch.cat = concate  -> cls_tokens ÏôÄ x Î•º Ïó∞Í≤∞Ìï®. (= cls ÌÜ†ÌÅ∞ Ï∂îÍ∞Ä Í≥ºÏ†ï.)\n",
    "        \n",
    "        # add position embedding\n",
    "        x += self.positions\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = PatchEmbedding()\n",
    "x = embed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape   #([8, 256, 972])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m values \u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(emb,emb)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#Linear projectionÏùÑ Í±∞Ïπú q,k,vÎ•º 8 Í∞úÏùò headÎ°ú ÎÇòÎàî\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m queries \u001b[38;5;241m=\u001b[39m rearrange(queries(\u001b[43mx\u001b[49m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb n (h d) -> b h n d\u001b[39m\u001b[38;5;124m'\u001b[39m, h\u001b[38;5;241m=\u001b[39mnum_heads)\n\u001b[1;32m     12\u001b[0m keys \u001b[38;5;241m=\u001b[39m rearrange(keys(x), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb n (h d) -> b h n d\u001b[39m\u001b[38;5;124m'\u001b[39m, h\u001b[38;5;241m=\u001b[39mnum_heads)\n\u001b[1;32m     13\u001b[0m values \u001b[38;5;241m=\u001b[39m rearrange(values(x), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb n (h d) -> b h n d\u001b[39m\u001b[38;5;124m'\u001b[39m, h\u001b[38;5;241m=\u001b[39mnum_heads)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "#----------------------------------------------------\n",
    "emb= 972\n",
    "num_heads =9  \n",
    "\n",
    "#k,q,v ÏûÖÎ†• Linear embedding=> Linear projection\n",
    "keys= nn.Linear(emb,emb)\n",
    "queries = nn.Linear(emb,emb)\n",
    "values =nn.Linear(emb,emb)\n",
    "\n",
    "#Linear projectionÏùÑ Í±∞Ïπú q,k,vÎ•º 8 Í∞úÏùò headÎ°ú ÎÇòÎàî\n",
    "queries = rearrange(queries(x), 'b n (h d) -> b h n d', h=num_heads)\n",
    "keys = rearrange(keys(x), 'b n (h d) -> b h n d', h=num_heads)\n",
    "values = rearrange(values(x), 'b n (h d) -> b h n d', h=num_heads)\n",
    "\n",
    "queries.shape, keys.shape, values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries * keys\n",
    "# Q.matmul(K.T)\n",
    "emb_size= 972   # 972\n",
    "\n",
    "energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
    "print(\"energy : \",energy.shape)\n",
    "\n",
    "#Get attention score\n",
    "scaling = emb_size**(1/2)\n",
    "att = F.softmax(energy, dim=-1)/scaling\n",
    "print(\"att : \", att.shape)\n",
    "\n",
    "#Attention score * Values\n",
    "out = torch.einsum('bhal, bhlv -> bhav',att, values)\n",
    "print(\"out : \", out.shape)\n",
    "\n",
    "#Rearrange to emb_size (concatenate)\n",
    "out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "print(\"out2 : \", out.shape)\n",
    "#---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >> Transformer Encoder\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "## Multihead attention.\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size: int = 972, num_heads: int = 9, dropout: float = 0):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        # fuse the queries, keys and values in one matrix\n",
    "        self.qkv = nn.Linear(emb_size, emb_size * 3)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "        \n",
    "    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n",
    "        # split keys, queries and values in num_heads\n",
    "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
    "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
    "        # sum up over the last axis\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len\n",
    "        #print(\"energy : \",energy.shape)\n",
    "        \n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "            \n",
    "        scaling = self.emb_size ** (1/2)   # Get attention score\n",
    "        att = F.softmax(energy, dim=-1) / scaling\n",
    "        att = self.att_drop(att)\n",
    "        #print(\"att : \", att.shape)\n",
    "        \n",
    "        # sum up over the third axis\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)   #Attention score * Values\n",
    "        #print(\"out : \", out.shape)\n",
    "\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")   #Rearrange to emb_size (concatenate)\n",
    "        #print(\"out2 : \", out.shape)\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Residuals\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "\n",
    "\n",
    "# MLP layer\n",
    "# Í∏∞ÌÉÄÏÇ¨Ìï≠, nn.Sequential ÏûÑÏúºÎ°ú Íµ≥Ïù¥ def forward Ïì∏ ÌïÑÏöîÍ∞Ä ÏóÜÏùå.\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >> Transformer Encoder Block\n",
    "\n",
    "\n",
    "## load custom module ============> Îî∞Î°ú Î∂àÎü¨Ïò§Í∏∞\n",
    "#from layers.Multihead_attention import MultiHeadAttention, ResidualAdd, FeedForwardBlock\n",
    "#from layers.patch_embedding import PatchEmbedding\n",
    "\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size: int = 972,\n",
    "                 drop_p: float = 0.,\n",
    "                 forward_expansion: int = 4,\n",
    "                 forward_drop_p: float = 0.,\n",
    "                 ** kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, **kwargs),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))\n",
    "        \n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth: int = 12, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "x = torch.randn(8,3,288,288)\n",
    "patches_embedded = PatchEmbedding()(x)\n",
    "TransformerEncoderBlock()(patches_embedded).shape\n",
    "#-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >> Make MLP Head\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size: int = 972, n_classes: int = 2):\n",
    "        super().__init__(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size), \n",
    "            nn.Linear(emb_size, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >> ÌïôÏäµ ÏΩîÎìú\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "# load custom module\n",
    "#from layers.patch_embedding import PatchEmbedding\n",
    "#from layers.Mlp_head import ClassificationHead\n",
    "#from layers.Earlystopping import EarlyStopping\n",
    "#from block.Encoder_Block import TransformerEncoder\n",
    "#from PIL import Image\n",
    "\n",
    "class ViT(nn.Sequential):\n",
    "    def __init__(self,     \n",
    "                in_channels: int = 3,\n",
    "                patch_size: int = 24,\n",
    "                emb_size: int = 972,\n",
    "                img_size: int = 288,\n",
    "                depth: int = 6,\n",
    "                n_classes: int = 2,\n",
    "                **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
    "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
    "            ClassificationHead(emb_size, n_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 972, 12, 12]       1,680,588\n",
      "         Rearrange-2             [-1, 144, 972]               0\n",
      "    PatchEmbedding-3             [-1, 145, 972]               0\n",
      "         LayerNorm-4             [-1, 145, 972]           1,944\n",
      "            Linear-5            [-1, 145, 2916]       2,837,268\n",
      "           Dropout-6          [-1, 9, 145, 145]               0\n",
      "            Linear-7             [-1, 145, 972]         945,756\n",
      "MultiHeadAttention-8             [-1, 145, 972]               0\n",
      "           Dropout-9             [-1, 145, 972]               0\n",
      "      ResidualAdd-10             [-1, 145, 972]               0\n",
      "        LayerNorm-11             [-1, 145, 972]           1,944\n",
      "           Linear-12            [-1, 145, 3888]       3,783,024\n",
      "             GELU-13            [-1, 145, 3888]               0\n",
      "          Dropout-14            [-1, 145, 3888]               0\n",
      "           Linear-15             [-1, 145, 972]       3,780,108\n",
      "          Dropout-16             [-1, 145, 972]               0\n",
      "      ResidualAdd-17             [-1, 145, 972]               0\n",
      "        LayerNorm-18             [-1, 145, 972]           1,944\n",
      "           Linear-19            [-1, 145, 2916]       2,837,268\n",
      "          Dropout-20          [-1, 9, 145, 145]               0\n",
      "           Linear-21             [-1, 145, 972]         945,756\n",
      "MultiHeadAttention-22             [-1, 145, 972]               0\n",
      "          Dropout-23             [-1, 145, 972]               0\n",
      "      ResidualAdd-24             [-1, 145, 972]               0\n",
      "        LayerNorm-25             [-1, 145, 972]           1,944\n",
      "           Linear-26            [-1, 145, 3888]       3,783,024\n",
      "             GELU-27            [-1, 145, 3888]               0\n",
      "          Dropout-28            [-1, 145, 3888]               0\n",
      "           Linear-29             [-1, 145, 972]       3,780,108\n",
      "          Dropout-30             [-1, 145, 972]               0\n",
      "      ResidualAdd-31             [-1, 145, 972]               0\n",
      "        LayerNorm-32             [-1, 145, 972]           1,944\n",
      "           Linear-33            [-1, 145, 2916]       2,837,268\n",
      "          Dropout-34          [-1, 9, 145, 145]               0\n",
      "           Linear-35             [-1, 145, 972]         945,756\n",
      "MultiHeadAttention-36             [-1, 145, 972]               0\n",
      "          Dropout-37             [-1, 145, 972]               0\n",
      "      ResidualAdd-38             [-1, 145, 972]               0\n",
      "        LayerNorm-39             [-1, 145, 972]           1,944\n",
      "           Linear-40            [-1, 145, 3888]       3,783,024\n",
      "             GELU-41            [-1, 145, 3888]               0\n",
      "          Dropout-42            [-1, 145, 3888]               0\n",
      "           Linear-43             [-1, 145, 972]       3,780,108\n",
      "          Dropout-44             [-1, 145, 972]               0\n",
      "      ResidualAdd-45             [-1, 145, 972]               0\n",
      "        LayerNorm-46             [-1, 145, 972]           1,944\n",
      "           Linear-47            [-1, 145, 2916]       2,837,268\n",
      "          Dropout-48          [-1, 9, 145, 145]               0\n",
      "           Linear-49             [-1, 145, 972]         945,756\n",
      "MultiHeadAttention-50             [-1, 145, 972]               0\n",
      "          Dropout-51             [-1, 145, 972]               0\n",
      "      ResidualAdd-52             [-1, 145, 972]               0\n",
      "        LayerNorm-53             [-1, 145, 972]           1,944\n",
      "           Linear-54            [-1, 145, 3888]       3,783,024\n",
      "             GELU-55            [-1, 145, 3888]               0\n",
      "          Dropout-56            [-1, 145, 3888]               0\n",
      "           Linear-57             [-1, 145, 972]       3,780,108\n",
      "          Dropout-58             [-1, 145, 972]               0\n",
      "      ResidualAdd-59             [-1, 145, 972]               0\n",
      "        LayerNorm-60             [-1, 145, 972]           1,944\n",
      "           Linear-61            [-1, 145, 2916]       2,837,268\n",
      "          Dropout-62          [-1, 9, 145, 145]               0\n",
      "           Linear-63             [-1, 145, 972]         945,756\n",
      "MultiHeadAttention-64             [-1, 145, 972]               0\n",
      "          Dropout-65             [-1, 145, 972]               0\n",
      "      ResidualAdd-66             [-1, 145, 972]               0\n",
      "        LayerNorm-67             [-1, 145, 972]           1,944\n",
      "           Linear-68            [-1, 145, 3888]       3,783,024\n",
      "             GELU-69            [-1, 145, 3888]               0\n",
      "          Dropout-70            [-1, 145, 3888]               0\n",
      "           Linear-71             [-1, 145, 972]       3,780,108\n",
      "          Dropout-72             [-1, 145, 972]               0\n",
      "      ResidualAdd-73             [-1, 145, 972]               0\n",
      "        LayerNorm-74             [-1, 145, 972]           1,944\n",
      "           Linear-75            [-1, 145, 2916]       2,837,268\n",
      "          Dropout-76          [-1, 9, 145, 145]               0\n",
      "           Linear-77             [-1, 145, 972]         945,756\n",
      "MultiHeadAttention-78             [-1, 145, 972]               0\n",
      "          Dropout-79             [-1, 145, 972]               0\n",
      "      ResidualAdd-80             [-1, 145, 972]               0\n",
      "        LayerNorm-81             [-1, 145, 972]           1,944\n",
      "           Linear-82            [-1, 145, 3888]       3,783,024\n",
      "             GELU-83            [-1, 145, 3888]               0\n",
      "          Dropout-84            [-1, 145, 3888]               0\n",
      "           Linear-85             [-1, 145, 972]       3,780,108\n",
      "          Dropout-86             [-1, 145, 972]               0\n",
      "      ResidualAdd-87             [-1, 145, 972]               0\n",
      "           Reduce-88                  [-1, 972]               0\n",
      "        LayerNorm-89                  [-1, 972]           1,944\n",
      "           Linear-90                    [-1, 2]           1,946\n",
      "================================================================\n",
      "Total params: 69,784,742\n",
      "Trainable params: 69,784,742\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.95\n",
      "Forward/backward pass size (MB): 166.73\n",
      "Params size (MB): 266.21\n",
      "Estimated Total Size (MB): 433.89\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(ViT(), (3,288,288), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Îç∞Ïù¥ÌÑ∞ ÌïôÏäµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split, WeightedRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from early import *  # Ï°∞Í∏∞ Ï¢ÖÎ£å ÏΩúÎ∞±\n",
    "\n",
    "# 1. Îç∞Ïù¥ÌÑ∞ Ï†ÑÏ≤òÎ¶¨ Ï†ïÏùò\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((288, 288), antialias=True),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "batch_size = 64\n",
    "data_dir = '/home/juhyun/Desktop/digitfinal/converted-dataset/train'  # class0/class1 Ìè¨Ìï®\n",
    "\n",
    "# 2. ImageFolderÎ°ú Ï†ÑÏ≤¥ Îç∞Ïù¥ÌÑ∞ÏÖã Î°úÎî©\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "classes = dataset.classes\n",
    "print(f\"ÌÅ¥ÎûòÏä§: {classes}\")  # Ïòà: ['class0', 'class1']\n",
    "\n",
    "# 3. 7:2:1 ÎπÑÏú®Î°ú Split\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.7 * total_size)\n",
    "val_size = int(0.2 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "trainset, valset, testset = random_split(dataset, [train_size, val_size, test_size])\n",
    "print(f\"‚û°Ô∏è Train: {len(trainset)}, Val: {len(valset)}, Test: {len(testset)}\")\n",
    "\n",
    "# ‚úÖ 4. WeightedRandomSampler: trainsetÎßå ÎåÄÏÉÅ\n",
    "train_targets = [dataset.targets[i] for i in trainset.indices]\n",
    "train_class_counts = Counter(train_targets)\n",
    "print(f\"Train Class Count: {train_class_counts}\")\n",
    "\n",
    "# ÌÅ¥ÎûòÏä§ ÎπÑÏú® Í∏∞Î∞ò Í∞ÄÏ§ëÏπò Í≥ÑÏÇ∞\n",
    "class_weights = 1. / torch.tensor([train_class_counts[0], train_class_counts[1]], dtype=torch.float)\n",
    "sample_weights = torch.tensor([class_weights[t] for t in train_targets])\n",
    "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# ‚úÖ 5. DataLoader Íµ¨ÏÑ±\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, sampler=sampler, num_workers=2)\n",
    "valloader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from early import * \n",
    "\n",
    "# ‚úÖ Î™®Îç∏/ÏÜêÏã§/ÏµúÏ†ÅÌôî ÏÑ§Ï†ï2\n",
    "device = torch.device('cuda:0')\n",
    "vit = ViT(in_channels=3, patch_size=18, emb_size=972, img_size=288, depth=12, n_classes=2).to(device)\n",
    "\n",
    "epochs = 1000\n",
    "lr = 0.001\n",
    "patience = 10\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(vit.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "os.makedirs('./pt', exist_ok=True)\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image, label = image.to(device), label.to(device).long()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Train Epoch: {epoch}[{batch_idx*len(image)}/{len(train_loader.dataset)}({100*batch_idx/len(train_loader):.0f}%)]\\t Train Loss : {loss.item():.6f}\")\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image, label = image.to(device), label.to(device).long()\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1)[1]\n",
    "            correct += prediction.eq(label).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ ÌïôÏäµ Î£®ÌîÑ\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(vit, trainloader, optimizer, log_interval=5)\n",
    "    val_loss, val_acc = evaluate(vit, valloader)\n",
    "    print(f\"\\n[Epoch: {epoch}],\\t Val Loss : {val_loss:.4f},\\t Val Accuracy : {val_acc:.2f} %\\n\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(vit.state_dict(), f'./pt/model_epoch_{epoch}_Accuracy_{val_acc:.2f}.pt')\n",
    "\n",
    "    early_stopping(val_loss, vit)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def evaluate_2(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image, label = image.to(device), label.to(device).long()\n",
    "            output = model(image)\n",
    "            loss = criterion(output, label)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            prob = torch.softmax(output, dim=1)[:, 1]  # ÌÅ¥ÎûòÏä§ 1Ïùò ÌôïÎ•†\n",
    "            pred = torch.argmax(output, dim=1)\n",
    "\n",
    "            all_probs.extend(prob.cpu().numpy())\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "            correct += pred.eq(label).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    # üß™ Ï∂îÍ∞Ä ÏßÄÌëú Í≥ÑÏÇ∞\n",
    "    precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "    try:\n",
    "        auroc = roc_auc_score(all_labels, all_probs)\n",
    "    except ValueError:\n",
    "        auroc = float('nan')  # ÌÅ¥ÎûòÏä§ 1Îßå ÏûàÎäî Í≤ΩÏö∞ Îì± ÏòàÏô∏ Ï≤òÎ¶¨\n",
    "\n",
    "    # üîô Î™®Îì† ÏßÄÌëú Î¶¨ÌÑ¥\n",
    "    return {\n",
    "        'loss': test_loss,\n",
    "        'accuracy': test_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auroc': auroc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç Final Test Evaluation...\")\n",
    "test_loss, test_acc = evaluate(vit, testloader)\n",
    "re= evaluate_2(vit, testloader)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n",
    "print(re)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
