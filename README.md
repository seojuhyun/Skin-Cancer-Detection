# ğŸ§  Vision Transformer (ViT) for Skin Cancer Detection  

ì´ í”„ë¡œì íŠ¸ëŠ” ë…¼ë¬¸ *"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (ICLR, 2021)"* ì—ì„œ ì œì•ˆëœ Vision Transformer(ViT) ì•„í‚¤í…ì²˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ISIC 2024 í”¼ë¶€ì•” ë°ì´í„°ì…‹ì—ì„œ ì–‘ì„±(Benign) / ì•…ì„±(Malignant) í”¼ë¶€ ë³‘ë³€ì„ ë¶„ë¥˜í•˜ê¸° ìœ„í•œ ëª¨ë¸ì„ êµ¬í˜„í•œ ê³¼ì œì…ë‹ˆë‹¤.

  <div align= 'center'>
  <img width="632" alt="image" src="https://github.com/user-attachments/assets/ec8a1aac-51dd-49cf-9d31-7eaeb1bed23d" />
  </div>




## ğŸ“ Dataset

- **ISIC 2024 - Skin Cancer Detection with 3D-TBP**  
  https://challenge2024.isic-archive.com/
- í´ë˜ìŠ¤: `benign (ì–‘ì„±)` / `malignant (ì•…ì„±)`
  
[benign (ì–‘ì„±)]

<p align="center">
  <img src="https://github.com/user-attachments/assets/73b92ce5-4d03-4043-aee1-b713189ae4cc" width="100">
  <img src="https://github.com/user-attachments/assets/1c45cb61-4c05-41e9-b23f-6cde032ca908" width="100"/>
</p>



[malignant (ì•…ì„±)]
  
<p align="center">
  <img src="https://github.com/user-attachments/assets/114ff4ec-7f65-44fd-b0ca-b4366b9922de" width="100">
  <img src="https://github.com/user-attachments/assets/45e67cdf-4469-4e42-ba3d-fcdcf41ecbaf" width="100"/>
</p>


- ì…ë ¥ ì´ë¯¸ì§€ í¬ê¸°: `288 Ã— 288`
- ë°ì´í„° ë¶ˆê· í˜• ì¡´ì¬ â†’ Train Class Count: {0: 240,389,  1: 246}
 



## ğŸ§  Model Architecture

- **Backbone**: Vision Transformer (ViT)
- Patch Size: `18`
- Channel: `3`
- Embedding Size: `128`
- Image Size: `288 Ã— 288`
- Depth (Transformer Layers): `8`
- Number of Classes: `2`


## ğŸ”¹ ì£¼ìš” êµ¬ì„± ìš”ì†Œ

- **Patch & Position Embedding**
- **Multi-Head Self-Attention**
  - Num Heads: `8`
  - Q/K/V Shape: `torch.Size([32, 8, 257, 16])`
- **Transformer Encoder**
  - Embedding Size: `128`
  - Feedforward Expansion: `Ã—4`



## ğŸ‹ï¸â€â™‚ï¸ Training Configuration

- Batch Size: `64`
- Loss Function: CrossEntropyLoss
- Evaluation Metric: AUC-ROC, Precision, Recall, F1-score



## ğŸ“Š Results

- **AUC-ROC**: `0.92`
- **Accuracy**: `0.99`
- **Recall**: `0.63`
- **Precision, F1-score**ëŠ” ë‚®ì€ í¸ (ì´ìƒ í´ë˜ìŠ¤ ë¹„ìœ¨ 0.001ë¡œ ì¸í•œ ì˜í–¥)
- ì „ì²´ì ìœ¼ë¡œ ì •ìƒ/ì´ìƒ êµ¬ë¶„ì„ ì˜ í•™ìŠµí•¨

> âš ï¸ ì´ìƒ í´ë˜ìŠ¤ì— ëŒ€í•œ ë¯¼ê°ë„(Recall)ëŠ” 63%ë¡œ ì–‘í˜¸í•˜ì§€ë§Œ, ë°ì´í„° ë¶ˆê· í˜•ìœ¼ë¡œ ì¸í•´ Precision ë° F1-scoreëŠ” ìƒëŒ€ì ìœ¼ë¡œ ë‚®ìŒ.



## ğŸ”¹ More Details

<div align= 'center'>  
  <img width="677" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-07-02 á„‹á…©á„’á…® 1 44 13" src="https://github.com/user-attachments/assets/af287246-9082-48fa-a669-fee1326c1bee" />
  <img width="676" alt="á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-07-02 á„‹á…©á„’á…® 1 44 59" src="https://github.com/user-attachments/assets/feb27549-1edc-4a93-94d3-2705751f06ec" />
</div>



## ğŸ“š References

- Dosovitskiy, Alexey, et al.  
  *An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.*  
  ICLR, 2021. [arXiv:2010.11929](https://arxiv.org/abs/2010.11929)

- Vaswani, Ashish, et al.  
  *Attention is All You Need.*  
  NeurIPS, 2017.


