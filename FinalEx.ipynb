{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VIT(vision transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://haystar.tistory.com/95\n",
    "\n",
    "https://ivelopalways.tistory.com/entry/Deep-Learning-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-Vision-TransformerViT-An-Image-is-Worth-16x16-Words-Transformers-for-Image-Recognition-at-Scale#google_vignette\n",
    "\n",
    "DATASET\n",
    "https://www.kaggle.com/competitions/isic-2024-challenge/data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# 경로 설정\n",
    "image_dir = '/home/juhyun/Downloads/isic-2024-challenge/train-image/image'\n",
    "meta_csv = '/home/juhyun/Downloads/isic-2024-challenge/train-metadata.csv'\n",
    "output_root = 'converted-dataset/train'\n",
    "\n",
    "# 메타데이터 로드\n",
    "df = pd.read_csv(meta_csv)\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    isic_id = row['isic_id']\n",
    "    label = str(row['target'])\n",
    "\n",
    "    # 입력 이미지 경로\n",
    "    src_path = os.path.join(image_dir, f\"{isic_id}.jpg\")\n",
    "    # 저장할 클래스 폴더\n",
    "    save_dir = os.path.join(output_root, f\"class{label}\")\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    # 출력 이미지 경로\n",
    "    dst_path = os.path.join(save_dir, f\"{isic_id}.jpg\")\n",
    "\n",
    "    try:\n",
    "        shutil.copyfile(src_path, dst_path)\n",
    "    except Exception as e:\n",
    "        print(f\" 복사 실패: {src_path} → {dst_path} : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "# 경로\n",
    "hdf5_path = '/home/juhyun/Downloads/isic-2024-challenge/test-image.hdf5'\n",
    "meta_csv = '/home/juhyun/Downloads/isic-2024-challenge/test-metadata.csv'\n",
    "output_dir = 'converted-dataset/test'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 메타데이터 로드\n",
    "df = pd.read_csv(meta_csv)\n",
    "\n",
    "# HDF5에서 이미지 추출\n",
    "with h5py.File(hdf5_path, 'r') as hdf:\n",
    "    for idx, row in df.iterrows():\n",
    "        isic_id = row['isic_id']\n",
    "\n",
    "        try:\n",
    "            raw = hdf[isic_id][()]  # shape: [[b'\\xff\\xd8...']]\n",
    "            if isinstance(raw, (np.ndarray, list)):\n",
    "                raw = raw[0]\n",
    "            img = Image.open(BytesIO(raw))\n",
    "            img.save(os.path.join(output_dir, f\"{isic_id}.jpg\"))\n",
    "        except Exception as e:\n",
    "            print(f\"❌ {isic_id} 변환 실패: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 값만 확인\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "x = Image.open('/home/juhyun/Desktop/digitfinal/converted-dataset/train/class0/ISIC_0015670.jpg')\n",
    "x = x.resize((288,288))\n",
    "tf_toTensor = ToTensor() \n",
    "x = tf_toTensor(x)\n",
    "x = torch.unsqueeze(x,0) #배치 크기 맞춰줌\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(8,3,288,288)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 18\n",
    "N = int(288*288/(18*18)) #256\n",
    "\n",
    "\"\"\"\n",
    "기존의 B*C*H*W의 차원을 B*N*(P*P*C)로 바꿔줘야함\n",
    "\n",
    "einops의 rearrange 함수를 이용하여 이미지를 패치로 나누고 flatten을 한번에 수행할 수 있다.\n",
    "\"\"\"\n",
    "\n",
    "patches = rearrange(x, 'b c (h s1) (w s2) -> b (h w) (s1 s2 c)', s1=P, s2=P) #[1,N,P*P*c\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >> PatchEmbedding, 파라미터는 직접 계산해서 지정함\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels: int = 3, patch_size: int = 18, emb_size: int = 972, img_size: int = 288):\n",
    "        self.patch_size = patch_size\n",
    "        super().__init__()\n",
    "        # patch embedding\n",
    "        self.projection = nn.Sequential(\n",
    "            # using a conv layer instead of a linear one -> performance gains\n",
    "            nn.Conv2d(in_channels, emb_size, kernel_size=patch_size, stride=patch_size),\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "        \n",
    "        # nn.Parameter = 학습 가능한 파라미터로 설정하는 것임.\n",
    "        # Add CLS Token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1, emb_size))\n",
    "        \n",
    "        # position embedding\n",
    "        self.positions = nn.Parameter(torch.randn((img_size // patch_size) **2 + 1, emb_size))\n",
    "\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        b, _, _, _ = x.shape\n",
    "        x = self.projection(x)\n",
    "        cls_tokens = repeat(self.cls_token, '() n e -> b n e', b=b)  # cls token을 x의 첫번째 차원으로 반복함.\n",
    "\n",
    "        # prepend the cls token to the input\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # torch.cat = concate  -> cls_tokens 와 x 를 연결함. (= cls 토큰 추가 과정.)\n",
    "        \n",
    "        # add position embedding\n",
    "        x += self.positions\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed = PatchEmbedding()\n",
    "x = embed(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape   #([8, 256, 972])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------\n",
    "emb= 972\n",
    "num_heads =9  \n",
    "\n",
    "#k,q,v 입력 Linear embedding=> Linear projection\n",
    "keys= nn.Linear(emb,emb)\n",
    "queries = nn.Linear(emb,emb)\n",
    "values =nn.Linear(emb,emb)\n",
    "\n",
    "#Linear projection을 거친 q,k,v를 8 개의 head로 나눔\n",
    "queries = rearrange(queries(x), 'b n (h d) -> b h n d', h=num_heads)\n",
    "keys = rearrange(keys(x), 'b n (h d) -> b h n d', h=num_heads)\n",
    "values = rearrange(values(x), 'b n (h d) -> b h n d', h=num_heads)\n",
    "\n",
    "queries.shape, keys.shape, values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries * keys\n",
    "# Q.matmul(K.T)\n",
    "emb_size= 972   # 972\n",
    "\n",
    "energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys)\n",
    "print(\"energy : \",energy.shape)\n",
    "\n",
    "#Get attention score\n",
    "scaling = emb_size**(1/2)\n",
    "att = F.softmax(energy, dim=-1)/scaling\n",
    "print(\"att : \", att.shape)\n",
    "\n",
    "#Attention score * Values\n",
    "out = torch.einsum('bhal, bhlv -> bhav',att, values)\n",
    "print(\"out : \", out.shape)\n",
    "\n",
    "#Rearrange to emb_size (concatenate)\n",
    "out = rearrange(out, \"b h n d -> b n (h d)\")\n",
    "print(\"out2 : \", out.shape)\n",
    "#---------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >> Transformer Encoder\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "## Multihead attention.\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, emb_size: int = 972, num_heads: int = 9, dropout: float = 0):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.num_heads = num_heads\n",
    "        # fuse the queries, keys and values in one matrix\n",
    "        self.qkv = nn.Linear(emb_size, emb_size * 3)\n",
    "        self.att_drop = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(emb_size, emb_size)\n",
    "        \n",
    "    def forward(self, x : Tensor, mask: Tensor = None) -> Tensor:\n",
    "        # split keys, queries and values in num_heads\n",
    "        qkv = rearrange(self.qkv(x), \"b n (h d qkv) -> (qkv) b h n d\", h=self.num_heads, qkv=3)\n",
    "        queries, keys, values = qkv[0], qkv[1], qkv[2]\n",
    "        # sum up over the last axis\n",
    "        energy = torch.einsum('bhqd, bhkd -> bhqk', queries, keys) # batch, num_heads, query_len, key_len\n",
    "        #print(\"energy : \",energy.shape)\n",
    "        \n",
    "        if mask is not None:\n",
    "            fill_value = torch.finfo(torch.float32).min\n",
    "            energy.mask_fill(~mask, fill_value)\n",
    "            \n",
    "        scaling = self.emb_size ** (1/2)   # Get attention score\n",
    "        att = F.softmax(energy, dim=-1) / scaling\n",
    "        att = self.att_drop(att)\n",
    "        #print(\"att : \", att.shape)\n",
    "        \n",
    "        # sum up over the third axis\n",
    "        out = torch.einsum('bhal, bhlv -> bhav ', att, values)   #Attention score * Values\n",
    "        #print(\"out : \", out.shape)\n",
    "\n",
    "        out = rearrange(out, \"b h n d -> b n (h d)\")   #Rearrange to emb_size (concatenate)\n",
    "        #print(\"out2 : \", out.shape)\n",
    "        out = self.projection(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# Residuals\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        \n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "\n",
    "\n",
    "# MLP layer\n",
    "# 기타사항, nn.Sequential 임으로 굳이 def forward 쓸 필요가 없음.\n",
    "class FeedForwardBlock(nn.Sequential):\n",
    "    def __init__(self, emb_size: int, expansion: int = 4, drop_p: float = 0.):\n",
    "        super().__init__(\n",
    "            nn.Linear(emb_size, expansion * emb_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(drop_p),\n",
    "            nn.Linear(expansion * emb_size, emb_size),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >> Transformer Encoder Block\n",
    "\n",
    "\n",
    "## load custom module ============> 따로 불러오기\n",
    "#from layers.Multihead_attention import MultiHeadAttention, ResidualAdd, FeedForwardBlock\n",
    "#from layers.patch_embedding import PatchEmbedding\n",
    "\n",
    "\n",
    "\n",
    "class TransformerEncoderBlock(nn.Sequential):\n",
    "    def __init__(self,\n",
    "                 emb_size: int = 972,\n",
    "                 drop_p: float = 0.,\n",
    "                 forward_expansion: int = 4,\n",
    "                 forward_drop_p: float = 0.,\n",
    "                 ** kwargs):\n",
    "        super().__init__(\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                MultiHeadAttention(emb_size, **kwargs),\n",
    "                nn.Dropout(drop_p)\n",
    "            )),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.LayerNorm(emb_size),\n",
    "                FeedForwardBlock(\n",
    "                    emb_size, expansion=forward_expansion, drop_p=forward_drop_p),\n",
    "                nn.Dropout(drop_p)\n",
    "            )\n",
    "            ))\n",
    "        \n",
    "class TransformerEncoder(nn.Sequential):\n",
    "    def __init__(self, depth: int = 12, **kwargs):\n",
    "        super().__init__(*[TransformerEncoderBlock(**kwargs) for _ in range(depth)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------\n",
    "x = torch.randn(8,3,288,288)\n",
    "patches_embedded = PatchEmbedding()(x)\n",
    "TransformerEncoderBlock()(patches_embedded).shape\n",
    "#-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >> Make MLP Head\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "class ClassificationHead(nn.Sequential):\n",
    "    def __init__(self, emb_size: int = 972, n_classes: int = 2):\n",
    "        super().__init__(\n",
    "            Reduce('b n e -> b e', reduction='mean'),\n",
    "            nn.LayerNorm(emb_size), \n",
    "            nn.Linear(emb_size, n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >> 학습 코드\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from PIL import Image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "\n",
    "# load custom module\n",
    "#from layers.patch_embedding import PatchEmbedding\n",
    "#from layers.Mlp_head import ClassificationHead\n",
    "#from layers.Earlystopping import EarlyStopping\n",
    "#from block.Encoder_Block import TransformerEncoder\n",
    "#from PIL import Image\n",
    "\n",
    "class ViT(nn.Sequential):\n",
    "    def __init__(self,     \n",
    "                in_channels: int = 3,\n",
    "                patch_size: int = 18,\n",
    "                emb_size: int = 972,\n",
    "                img_size: int = 288,\n",
    "                depth: int = 12,\n",
    "                n_classes: int = 2,\n",
    "                **kwargs):\n",
    "        super().__init__(\n",
    "            PatchEmbedding(in_channels, patch_size, emb_size, img_size),\n",
    "            TransformerEncoder(depth, emb_size=emb_size, **kwargs),\n",
    "            ClassificationHead(emb_size, n_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 972, 16, 16]         945,756\n",
      "         Rearrange-2             [-1, 256, 972]               0\n",
      "    PatchEmbedding-3             [-1, 257, 972]               0\n",
      "         LayerNorm-4             [-1, 257, 972]           1,944\n",
      "            Linear-5            [-1, 257, 2916]       2,837,268\n",
      "           Dropout-6          [-1, 9, 257, 257]               0\n",
      "            Linear-7             [-1, 257, 972]         945,756\n",
      "MultiHeadAttention-8             [-1, 257, 972]               0\n",
      "           Dropout-9             [-1, 257, 972]               0\n",
      "      ResidualAdd-10             [-1, 257, 972]               0\n",
      "        LayerNorm-11             [-1, 257, 972]           1,944\n",
      "           Linear-12            [-1, 257, 3888]       3,783,024\n",
      "             GELU-13            [-1, 257, 3888]               0\n",
      "          Dropout-14            [-1, 257, 3888]               0\n",
      "           Linear-15             [-1, 257, 972]       3,780,108\n",
      "          Dropout-16             [-1, 257, 972]               0\n",
      "      ResidualAdd-17             [-1, 257, 972]               0\n",
      "        LayerNorm-18             [-1, 257, 972]           1,944\n",
      "           Linear-19            [-1, 257, 2916]       2,837,268\n",
      "          Dropout-20          [-1, 9, 257, 257]               0\n",
      "           Linear-21             [-1, 257, 972]         945,756\n",
      "MultiHeadAttention-22             [-1, 257, 972]               0\n",
      "          Dropout-23             [-1, 257, 972]               0\n",
      "      ResidualAdd-24             [-1, 257, 972]               0\n",
      "        LayerNorm-25             [-1, 257, 972]           1,944\n",
      "           Linear-26            [-1, 257, 3888]       3,783,024\n",
      "             GELU-27            [-1, 257, 3888]               0\n",
      "          Dropout-28            [-1, 257, 3888]               0\n",
      "           Linear-29             [-1, 257, 972]       3,780,108\n",
      "          Dropout-30             [-1, 257, 972]               0\n",
      "      ResidualAdd-31             [-1, 257, 972]               0\n",
      "        LayerNorm-32             [-1, 257, 972]           1,944\n",
      "           Linear-33            [-1, 257, 2916]       2,837,268\n",
      "          Dropout-34          [-1, 9, 257, 257]               0\n",
      "           Linear-35             [-1, 257, 972]         945,756\n",
      "MultiHeadAttention-36             [-1, 257, 972]               0\n",
      "          Dropout-37             [-1, 257, 972]               0\n",
      "      ResidualAdd-38             [-1, 257, 972]               0\n",
      "        LayerNorm-39             [-1, 257, 972]           1,944\n",
      "           Linear-40            [-1, 257, 3888]       3,783,024\n",
      "             GELU-41            [-1, 257, 3888]               0\n",
      "          Dropout-42            [-1, 257, 3888]               0\n",
      "           Linear-43             [-1, 257, 972]       3,780,108\n",
      "          Dropout-44             [-1, 257, 972]               0\n",
      "      ResidualAdd-45             [-1, 257, 972]               0\n",
      "        LayerNorm-46             [-1, 257, 972]           1,944\n",
      "           Linear-47            [-1, 257, 2916]       2,837,268\n",
      "          Dropout-48          [-1, 9, 257, 257]               0\n",
      "           Linear-49             [-1, 257, 972]         945,756\n",
      "MultiHeadAttention-50             [-1, 257, 972]               0\n",
      "          Dropout-51             [-1, 257, 972]               0\n",
      "      ResidualAdd-52             [-1, 257, 972]               0\n",
      "        LayerNorm-53             [-1, 257, 972]           1,944\n",
      "           Linear-54            [-1, 257, 3888]       3,783,024\n",
      "             GELU-55            [-1, 257, 3888]               0\n",
      "          Dropout-56            [-1, 257, 3888]               0\n",
      "           Linear-57             [-1, 257, 972]       3,780,108\n",
      "          Dropout-58             [-1, 257, 972]               0\n",
      "      ResidualAdd-59             [-1, 257, 972]               0\n",
      "        LayerNorm-60             [-1, 257, 972]           1,944\n",
      "           Linear-61            [-1, 257, 2916]       2,837,268\n",
      "          Dropout-62          [-1, 9, 257, 257]               0\n",
      "           Linear-63             [-1, 257, 972]         945,756\n",
      "MultiHeadAttention-64             [-1, 257, 972]               0\n",
      "          Dropout-65             [-1, 257, 972]               0\n",
      "      ResidualAdd-66             [-1, 257, 972]               0\n",
      "        LayerNorm-67             [-1, 257, 972]           1,944\n",
      "           Linear-68            [-1, 257, 3888]       3,783,024\n",
      "             GELU-69            [-1, 257, 3888]               0\n",
      "          Dropout-70            [-1, 257, 3888]               0\n",
      "           Linear-71             [-1, 257, 972]       3,780,108\n",
      "          Dropout-72             [-1, 257, 972]               0\n",
      "      ResidualAdd-73             [-1, 257, 972]               0\n",
      "        LayerNorm-74             [-1, 257, 972]           1,944\n",
      "           Linear-75            [-1, 257, 2916]       2,837,268\n",
      "          Dropout-76          [-1, 9, 257, 257]               0\n",
      "           Linear-77             [-1, 257, 972]         945,756\n",
      "MultiHeadAttention-78             [-1, 257, 972]               0\n",
      "          Dropout-79             [-1, 257, 972]               0\n",
      "      ResidualAdd-80             [-1, 257, 972]               0\n",
      "        LayerNorm-81             [-1, 257, 972]           1,944\n",
      "           Linear-82            [-1, 257, 3888]       3,783,024\n",
      "             GELU-83            [-1, 257, 3888]               0\n",
      "          Dropout-84            [-1, 257, 3888]               0\n",
      "           Linear-85             [-1, 257, 972]       3,780,108\n",
      "          Dropout-86             [-1, 257, 972]               0\n",
      "      ResidualAdd-87             [-1, 257, 972]               0\n",
      "        LayerNorm-88             [-1, 257, 972]           1,944\n",
      "           Linear-89            [-1, 257, 2916]       2,837,268\n",
      "          Dropout-90          [-1, 9, 257, 257]               0\n",
      "           Linear-91             [-1, 257, 972]         945,756\n",
      "MultiHeadAttention-92             [-1, 257, 972]               0\n",
      "          Dropout-93             [-1, 257, 972]               0\n",
      "      ResidualAdd-94             [-1, 257, 972]               0\n",
      "        LayerNorm-95             [-1, 257, 972]           1,944\n",
      "           Linear-96            [-1, 257, 3888]       3,783,024\n",
      "             GELU-97            [-1, 257, 3888]               0\n",
      "          Dropout-98            [-1, 257, 3888]               0\n",
      "           Linear-99             [-1, 257, 972]       3,780,108\n",
      "         Dropout-100             [-1, 257, 972]               0\n",
      "     ResidualAdd-101             [-1, 257, 972]               0\n",
      "       LayerNorm-102             [-1, 257, 972]           1,944\n",
      "          Linear-103            [-1, 257, 2916]       2,837,268\n",
      "         Dropout-104          [-1, 9, 257, 257]               0\n",
      "          Linear-105             [-1, 257, 972]         945,756\n",
      "MultiHeadAttention-106             [-1, 257, 972]               0\n",
      "         Dropout-107             [-1, 257, 972]               0\n",
      "     ResidualAdd-108             [-1, 257, 972]               0\n",
      "       LayerNorm-109             [-1, 257, 972]           1,944\n",
      "          Linear-110            [-1, 257, 3888]       3,783,024\n",
      "            GELU-111            [-1, 257, 3888]               0\n",
      "         Dropout-112            [-1, 257, 3888]               0\n",
      "          Linear-113             [-1, 257, 972]       3,780,108\n",
      "         Dropout-114             [-1, 257, 972]               0\n",
      "     ResidualAdd-115             [-1, 257, 972]               0\n",
      "       LayerNorm-116             [-1, 257, 972]           1,944\n",
      "          Linear-117            [-1, 257, 2916]       2,837,268\n",
      "         Dropout-118          [-1, 9, 257, 257]               0\n",
      "          Linear-119             [-1, 257, 972]         945,756\n",
      "MultiHeadAttention-120             [-1, 257, 972]               0\n",
      "         Dropout-121             [-1, 257, 972]               0\n",
      "     ResidualAdd-122             [-1, 257, 972]               0\n",
      "       LayerNorm-123             [-1, 257, 972]           1,944\n",
      "          Linear-124            [-1, 257, 3888]       3,783,024\n",
      "            GELU-125            [-1, 257, 3888]               0\n",
      "         Dropout-126            [-1, 257, 3888]               0\n",
      "          Linear-127             [-1, 257, 972]       3,780,108\n",
      "         Dropout-128             [-1, 257, 972]               0\n",
      "     ResidualAdd-129             [-1, 257, 972]               0\n",
      "       LayerNorm-130             [-1, 257, 972]           1,944\n",
      "          Linear-131            [-1, 257, 2916]       2,837,268\n",
      "         Dropout-132          [-1, 9, 257, 257]               0\n",
      "          Linear-133             [-1, 257, 972]         945,756\n",
      "MultiHeadAttention-134             [-1, 257, 972]               0\n",
      "         Dropout-135             [-1, 257, 972]               0\n",
      "     ResidualAdd-136             [-1, 257, 972]               0\n",
      "       LayerNorm-137             [-1, 257, 972]           1,944\n",
      "          Linear-138            [-1, 257, 3888]       3,783,024\n",
      "            GELU-139            [-1, 257, 3888]               0\n",
      "         Dropout-140            [-1, 257, 3888]               0\n",
      "          Linear-141             [-1, 257, 972]       3,780,108\n",
      "         Dropout-142             [-1, 257, 972]               0\n",
      "     ResidualAdd-143             [-1, 257, 972]               0\n",
      "       LayerNorm-144             [-1, 257, 972]           1,944\n",
      "          Linear-145            [-1, 257, 2916]       2,837,268\n",
      "         Dropout-146          [-1, 9, 257, 257]               0\n",
      "          Linear-147             [-1, 257, 972]         945,756\n",
      "MultiHeadAttention-148             [-1, 257, 972]               0\n",
      "         Dropout-149             [-1, 257, 972]               0\n",
      "     ResidualAdd-150             [-1, 257, 972]               0\n",
      "       LayerNorm-151             [-1, 257, 972]           1,944\n",
      "          Linear-152            [-1, 257, 3888]       3,783,024\n",
      "            GELU-153            [-1, 257, 3888]               0\n",
      "         Dropout-154            [-1, 257, 3888]               0\n",
      "          Linear-155             [-1, 257, 972]       3,780,108\n",
      "         Dropout-156             [-1, 257, 972]               0\n",
      "     ResidualAdd-157             [-1, 257, 972]               0\n",
      "       LayerNorm-158             [-1, 257, 972]           1,944\n",
      "          Linear-159            [-1, 257, 2916]       2,837,268\n",
      "         Dropout-160          [-1, 9, 257, 257]               0\n",
      "          Linear-161             [-1, 257, 972]         945,756\n",
      "MultiHeadAttention-162             [-1, 257, 972]               0\n",
      "         Dropout-163             [-1, 257, 972]               0\n",
      "     ResidualAdd-164             [-1, 257, 972]               0\n",
      "       LayerNorm-165             [-1, 257, 972]           1,944\n",
      "          Linear-166            [-1, 257, 3888]       3,783,024\n",
      "            GELU-167            [-1, 257, 3888]               0\n",
      "         Dropout-168            [-1, 257, 3888]               0\n",
      "          Linear-169             [-1, 257, 972]       3,780,108\n",
      "         Dropout-170             [-1, 257, 972]               0\n",
      "     ResidualAdd-171             [-1, 257, 972]               0\n",
      "          Reduce-172                  [-1, 972]               0\n",
      "       LayerNorm-173                  [-1, 972]           1,944\n",
      "          Linear-174                    [-1, 2]           1,946\n",
      "================================================================\n",
      "Total params: 137,150,174\n",
      "Trainable params: 137,150,174\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.95\n",
      "Forward/backward pass size (MB): 609.03\n",
      "Params size (MB): 523.19\n",
      "Estimated Total Size (MB): 1133.16\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(ViT(), (3,288,288), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, random_split, WeightedRandomSampler\n",
    "from torchvision import datasets, transforms\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from early import *  # 조기 종료 콜백\n",
    "\n",
    "# 1. 데이터 전처리 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((288, 288), antialias=True),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "batch_size = 64\n",
    "data_dir = '/home/juhyun/Desktop/digitfinal/converted-dataset/train'  # class0/class1 포함\n",
    "\n",
    "# 2. ImageFolder로 전체 데이터셋 로딩\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "classes = dataset.classes\n",
    "print(f\"클래스: {classes}\")  # 예: ['class0', 'class1']\n",
    "\n",
    "# 3. 7:2:1 비율로 Split\n",
    "total_size = len(dataset)\n",
    "train_size = int(0.7 * total_size)\n",
    "val_size = int(0.2 * total_size)\n",
    "test_size = total_size - train_size - val_size\n",
    "\n",
    "trainset, valset, testset = random_split(dataset, [train_size, val_size, test_size])\n",
    "print(f\"➡️ Train: {len(trainset)}, Val: {len(valset)}, Test: {len(testset)}\")\n",
    "\n",
    "# ✅ 4. WeightedRandomSampler: trainset만 대상\n",
    "train_targets = [dataset.targets[i] for i in trainset.indices]\n",
    "train_class_counts = Counter(train_targets)\n",
    "print(f\"Train Class Count: {train_class_counts}\")\n",
    "\n",
    "# 클래스 비율 기반 가중치 계산\n",
    "class_weights = 1. / torch.tensor([train_class_counts[0], train_class_counts[1]], dtype=torch.float)\n",
    "sample_weights = torch.tensor([class_weights[t] for t in train_targets])\n",
    "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "# ✅ 5. DataLoader 구성\n",
    "trainloader = DataLoader(trainset, batch_size=batch_size, sampler=sampler, num_workers=2)\n",
    "valloader = DataLoader(valset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from early import * \n",
    "\n",
    "# ✅ 모델/손실/최적화 설정2\n",
    "device = torch.device('cuda:0')\n",
    "vit = ViT(in_channels=3, patch_size=18, emb_size=972, img_size=288, depth=12, n_classes=2).to(device)\n",
    "\n",
    "epochs = 1000\n",
    "lr = 0.001\n",
    "patience = 10\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(vit.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "os.makedirs('./pt', exist_ok=True)\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "\n",
    "\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image, label = image.to(device), label.to(device).long()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Train Epoch: {epoch}[{batch_idx*len(image)}/{len(train_loader.dataset)}({100*batch_idx/len(train_loader):.0f}%)]\\t Train Loss : {loss.item():.6f}\")\n",
    "\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image, label = image.to(device), label.to(device).long()\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1)[1]\n",
    "            correct += prediction.eq(label).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✅ 학습 루프\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(vit, trainloader, optimizer, log_interval=5)\n",
    "    val_loss, val_acc = evaluate(vit, valloader)\n",
    "    print(f\"\\n[Epoch: {epoch}],\\t Val Loss : {val_loss:.4f},\\t Val Accuracy : {val_acc:.2f} %\\n\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(vit.state_dict(), f'./pt/model_epoch_{epoch}_Accuracy_{val_acc:.2f}.pt')\n",
    "\n",
    "    early_stopping(val_loss, vit)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def evaluate_2(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image, label = image.to(device), label.to(device).long()\n",
    "            output = model(image)\n",
    "            loss = criterion(output, label)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            prob = torch.softmax(output, dim=1)[:, 1]  # 클래스 1의 확률\n",
    "            pred = torch.argmax(output, dim=1)\n",
    "\n",
    "            all_probs.extend(prob.cpu().numpy())\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "            correct += pred.eq(label).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    # 🧪 추가 지표 계산\n",
    "    precision = precision_score(all_labels, all_preds, zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "    try:\n",
    "        auroc = roc_auc_score(all_labels, all_probs)\n",
    "    except ValueError:\n",
    "        auroc = float('nan')  # 클래스 1만 있는 경우 등 예외 처리\n",
    "\n",
    "    # 🔙 모든 지표 리턴\n",
    "    return {\n",
    "        'loss': test_loss,\n",
    "        'accuracy': test_accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auroc': auroc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔍 Final Test Evaluation...\")\n",
    "test_loss, test_acc = evaluate(vit, testloader)\n",
    "re= evaluate_2(vit, testloader)\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%\")\n",
    "print(re)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
